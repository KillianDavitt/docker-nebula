<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="normalize.css" type="text/css" />
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#docker-swarm">7 Docker Swarm</a><ul>
<li><a href="#creating-a-swarm-cluster">7.1 Creating a Swarm cluster</a></li>
<li><a href="#start-and-test-the-swarm-manager">7.2 Start and test the swarm manager</a></li>
<li><a href="#test-the-swarm">7.3 Test the swarm</a></li>
<li><a href="#deploy-a-container-to-the-swarm">Deploy a container to the swarm</a></li>
</ul></li>
</ul>
</div>
<h1 id="docker-swarm">7 Docker Swarm</h1>
<p>Docker swarm is a clustering technology for docker that turns a pool of docker hosts (by which in our context, we mean SCSSNebula virtual machines running the docker service) into a single virtual docker host. You can direct your docker client to communicate with a swarm such that containers you request to be run may be run on any of the docker hosts within the pool of hosts managed by the swarm. This makes is relatively easy to deploy and manage highly scalable network applications. If one needs more container instances, one simply runs them and docker swarm will take care of where best to run them. If our pool of docker hosts become overloaded, we can simply add new hosts to the swarm.This architectural model is the basis of most real-world scalable service deployments today: behind some well known service endpoint, often accessible via a web browser, sits a dynamic group of tens, hundreds or even thousands of collaborating nodes.</p>
<h2 id="creating-a-swarm-cluster">7.1 Creating a Swarm cluster</h2>
<p>To proceed you must have created a set of nodes in the SCSSNebula cloud, and configured your machine set to use TLS as described in the Master-Worker sequence above, or by having created all nodes using <code>docker-machine</code>. In the following text, we will presume that one of the machines is designated as the swarm master, another machine will act a secondary master and provide something called a <em>discovery backend</em> service, and all other machines will be workers in the swarm. Whenever we refer to a client machine, this means a machine with docker installed from which the <em>docker client</em> is used to invoke commands on a the swarm, possibly hosted on a remote machine in the SCSSNebula, but not necessarily so.</p>
<ol style="list-style-type: decimal">
<li><p>Create a discovery backend.</p>
<p>To begin the swarm configuration for your machine set, we first create a <em>discovery backend</em> which is a service that lets swarm members find each other. Login to your registry host (this will be a lightly loaded host suitable to also act as a discovery backend) or create a node for the purpose. We shall refer to this node in the following text as the <code>registry</code>. Create the discovery backend as follows:</p>
<pre><code>$ docker run -d --restart=always -p 8500:8500 --name=consul progrium/consul -server -bootstrap
Unable to find image &#39;progrium/consul:latest&#39; locally
latest: Pulling from progrium/consul
c862d82a67a2: Pull complete 
            ...
5d1cefca2a28: Pull complete 
Digest: sha256:8cc8023462905929df9a79ff67ee435a36848ce7a10f18d6d0faba9306b97274
Status: Downloaded newer image for progrium/consul:latest
d71dfcd5f349d4404d1996aedddc38147c2c4fd2a929242f8c247cd22bfa4a4c</code></pre>
<p>Run the command <code>docker ps</code> to verify that the discovery backend is running.</p></li>
<li><p>Create and distribute TLS keys</p>
<p>To secure the swarm, we need to generate a set of TLS key pairs for the managers and swam nodes. These will be used for the swarm only, by the swarm software containers, and are not to be confused with TLS configuration you may have performed to secure docker client to engine communication.</p>
<p>As we probably want to perform this across a set of nodes, for simplicity, the following script collects all relevant operations for node configuration, such that executing <code>swarm-config.sh NODENAME ca.pem ca-priv-key.pem</code> will configure the node NODENAME:</p>
<p>Download the script</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">wget</span> https://raw.githubusercontent.com/KillianDavitt/docker-nebula/master/scripts/swarm-config.sh</code></pre></div>
<p>If you have followed the steps outlined, you now have compatible certificates installed on all machines, with the certificate authority that generated these certificates trusted by all machines. We are now ready to run the swam software on our nodes, using these certificates to secure host-to-host communication.</p></li>
<li><p>Create swarm managers.</p>
<p>In a production environment, you should create two swarm managers and configure them to work together to maintain the cluster. So lets do this. We will create a primary manager on the <code>docker-machine</code> node (note that this must be a node in the SCSSNebula cloud) and a secondary manager on the <code>registry</code> node (also an SCSSNebula node).</p>
<p>Login to your <code>docker-machine</code> node (or select a docker host to act as swarm master manager) and perform the following:</p>
<pre><code>$ export IPADR=$(ifconfig eth0 | grep -Eo &#39;inet (addr:)?([0-9]*\.){3}[0-9]*&#39; | grep -Eo &#39;([0-9]*\.){3}[0-9]*&#39; | grep -v &#39;127.0.0.1&#39;)
# $ docker run -d --restart=always --name=swarm_mgr -p 4000:4000 swarm manage -H :4000 --replication --advertise $IPADR:4000 consul://$(docker-machine ip registry):8500

$ docker run -d --restart=always --name=swarm_mgr -p 4000:4000 -v /var/lib/boot2docker/swarm-certs:/server:ro swarm manage --tlsverify --tlscacert=/server/ca.pem --tlscert=/server/cert.pem --tlskey=/server/key.pem --host=0.0.0.0:4000 --replication --advertise $IPADR:4000 consul://$(docker-machine ip registry):8500
    ....
$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                              NAMES
fb8feaa84042        swarm               &quot;/swarm manage -H :40&quot;   2 minutes ago      Up 2 minutes       2375/tcp, 0.0.0.0:4000-&gt;4000/tcp   swarm_mgr</code></pre>
<p>The first command retrieves the IP address of the machine and sets and environment variable which is then used in the command to create the swarm manager. Recall that our discovery service is running on the <code>registry</code> node, and this is why the ip of that node is used to configure the consul parameter. The <code>docker ps</code> command lists the running swarm manager process.</p>
<p>Next we will create a second swarm manager on the registry node (the same node that incidentally on which we have the discovery backend running). On the docker-machine node, execute the following:</p>
<pre><code>$ eval $(docker-machine env registry)
# $ docker run -d --restart=always --name=swarm_bck_mgr -p 4000:4000 swarm manage -H :4000 --replication --advertise $(docker-machine ip registry):4000 consul://$(docker-machine ip registry):8500

$ docker run -d --restart=always --name=swarm_bck_mgr -p 4000:4000 -v /var/lib/boot2docker/swarm-certs:/server:ro swarm manage --tlsverify --tlscacert=/server/ca.pem --tlscert=/server/cert.pem --tlskey=/server/key.pem --host=0.0.0.0:4000 --replication --advertise $(docker-machine ip registry):4000 consul://$(docker-machine ip registry):8500


$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                            NAMES
6274fa6384a8        swarm               &quot;/swarm manage -H :40&quot;   1 seconds ago       Up 2 seconds        2375/tcp, 0.0.0.0:4000-&gt;4000/tcp                                                 swarm_bck_mgr
d71dfcd5f349        progrium/consul     &quot;/bin/start -server -&quot;   35 minutes ago      Up 35 minutes       53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500-&gt;8500/tcp   consul
9ae624d42b58        registry:2          &quot;/entrypoint.sh /etc/&quot;   5 hours ago         Up 5 hours          0.0.0.0:443-&gt;5000/tcp                                                            registry</code></pre>
<p>We now have two managers and a discovery backend service running. Next, we will add our worker nodes to the swarm.</p></li>
<li><p>Join each docker worker machine to the swarm cluster by executing the following command, once for each worker machine, replacing <em>MACHINE_NAME</em> with the docker-machine name of the relevant worker node. Note that these commands are run from the docker-machine node in each case. We use the <code>eval $(docker-machine env MACHINE_NAME)</code> command to configure our local docker engine client to communicate with the remote node’s docker engine. Note that we configure each swarm member to use the swarm certificates we have deployed to the host.</p>
<p><em>Be sure to specify TCP port 2376 and not 2375: we are using TLS based access. ???</em></p>
<pre><code>$ eval $(docker-machine env MACHINE_NAME)
$ docker run --restart=always -d -v /var/lib/boot2docker/swarm-certs:/server:ro swarm join --discovery-opt kv.cacertfile=/server/ca.pem --discovery-opt kv.certfile=/server/cert.pem --discovery-opt kv.keyfile=/server/key.pem --host=0.0.0.0:4000 --advertise=$(docker-machine ip MACHINE_NAME):4000 consul://$(docker-machine ip registry):8500
            ...
$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
f58c9796faf2        swarm               &quot;/swarm join --advert&quot;   4 seconds ago       Up 5 seconds        2375/tcp            suspicious_noyce</code></pre>
<p><em>Recall that this communication operates over a secure TLS channel and that the primary advantage of docker-machine to us here is to manage this aspect of client/server communication for us. If you have not used docker-machine to build your nodes, then you will have to manually configure your docker engine client, providing it with the appropriate TLS certificates and so forth, by manual environment variable setting. This is error prone. You could alternatively login to each node in turn and perform the <code>docker run</code> command locally using that node’s docker engine client. However, generally we try to avoid having to manually login to machines to perform actions as this approach is hard automate in scripts: far better in general to manage your systems from a centralised point. Draw your own conclusions.</em></p></li>
</ol>
<p>We now have a set of worker machines registered to a docker swarm.</p>
<h2 id="start-and-test-the-swarm-manager">7.2 Start and test the swarm manager</h2>
<p>Start the swarm manager on your master machine with the following command to launch a new container with TLS enabled:</p>
<pre><code>$ docker run -d -p 3376:3376 -v /etc/docker/ssl:/server:ro swarm manage --tlsverify --tlscacert=/server/ca.pem --tlscert=/server/cert.pem --tlskey=/server/key.pem --host=0.0.0.0:3376 token://$TOKEN</code></pre>
<p>The command above launches a new container based on the swarm image and it maps port 3376 on the server to port 3376 inside the container. This mapping ensures that Docker Engine commands sent to the host on port 3376 are passed on to port 3376 inside the container. The container runs the Swarm manage process with the –tlsverify, –tlscacert, –tlscert and –tlskey options specified. These options force TLS verification and specify the location of the Swarm manager’s TLS keys, which we point to those we previously created in <code>/etc/docker/ssl</code> for the docker service on the master (we are following a policy of having a single set of TLS keys for each virtual machine’s docker service, and all containers running within it. We could instead choose to have unique TLS keys for each installed container, but that would be overkill here).</p>
<p>Next run <code>docker ps</code> to verify the swarm manager is running:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">docker</span> ps
<span class="kw">CONTAINER</span> ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                              NAMES
<span class="kw">035dbf57b26e</span>   swarm               <span class="st">&quot;/swarm manage --tlsv&quot;</span>   7 seconds ago    Up 7 seconds    2375/tcp, 0.0.0.0:3376-<span class="kw">&gt;</span>3376/tcp   compassionate_lovelace</code></pre></div>
<p>We now have a swarm cluster, configured to use TLS. Next, we will test the configuration.</p>
<h2 id="test-the-swarm">7.3 Test the swarm</h2>
<p>Lets try to use our cluster. On the master, type the following:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">docker</span> -H tcp://127.0.0.1:3376 version
<span class="kw">Client</span>:
 <span class="kw">Version</span>:      1.9.1
 <span class="kw">API</span> version:  1.21
 <span class="kw">Go</span> version:   go1.4.2
 <span class="kw">Git</span> commit:   a34a1d5
 <span class="kw">Built</span>:        Fri Nov 20 13:12:04 UTC 2015
 <span class="kw">OS</span>/Arch:      <span class="kw">linux/amd64</span>

<span class="kw">Server</span>:
 <span class="kw">Version</span>:      swarm/1.0.1
 <span class="kw">API</span> version:  1.21
 <span class="kw">Go</span> version:   go1.5.2
 <span class="kw">Git</span> commit:   744e3a3
 <span class="kw">Built</span>:
 <span class="kw">OS</span>/Arch:      <span class="kw">linux/amd64</span></code></pre></div>
<p>Note the port number <code>3376</code>, which is the port number of the docker swarm container that we launched on the local docker engine. The local docker engine continues to accept connections on port <code>2376</code>. Do not confuse the two.</p>
<p>The output above shows the Server version as “swarm/1.0.1”. This means that the command was successfully issued against the Swarm manager. If instead, you get a response like:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">docker</span> -H tcp://127.0.0.1:3376 version
<span class="kw">:</span>
 <span class="kw">Version</span>:      1.9.1
 <span class="kw">API</span> version:  1.21
 <span class="kw">Go</span> version:   go1.4.2
 <span class="kw">Git</span> commit:   a34a1d5
 <span class="kw">Built</span>:        Fri Nov 20 13:12:04 UTC 2015
 <span class="kw">OS</span>/Arch:      <span class="kw">linux/amd64</span>
<span class="kw">Get</span> http://127.0.0.1:3376/v1.21/version: malformed HTTP response <span class="st">&quot;\x15\x03\x01\x00\x02\x02&quot;</span>.
<span class="kw">*</span> Are you trying to connect to a TLS-enabled daemon without TLS?</code></pre></div>
<p>then you do not have TLS enabled for the master machine docker client. Perhaps you did not persist the DOCKER_TLS_VERIFY, DOCKER_CERT_PATH, and DOCKER_HOST environment variables from a previous step. Consult the documentation above regarding configuring TLS or try instead the following alternative command form, which provides values for these variables on the command line:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">docker</span> --tlsverify --tlscacert=~/.docker/ca.pem --tlscert=~/.docker/cert.pem --tlskey=~/.docker/key.pem -H tcp://127.0.0.1:3376 version</code></pre></div>
<p><em>Note that if you were to perform these commands on a TLS enabled client machine (your laptop perhaps), then you would replace <code>127.0.0.1</code> in the aforementioned commands with the IP address of the master machine. In general if DOCKER_HOST is set, then you do not need to include the <code>-H tcp://a.b.c.d:n</code> option (unless you wish to override the environment variable setting).</em></p>
<h2 id="deploy-a-container-to-the-swarm">Deploy a container to the swarm</h2>
<p>Now that we have a fully functional docker swarm, it remains to run some simple containers on the swarm and observe the load balancing of docker swarm in action. We will address all further commands to the master node’s docker engine. You can achieve this by logging into this machine, or any other with TLS enabled network access to the master, and ensuring that the docker client is directed to communicate with the master’s docker swarm manager, either by setting the environment variable DOCKER_HOST or setting a <code>-H</code> command line option as described above. In the following we will assume the DOCKER_HOST variable is set correctly.</p>
<p><em>Note though, that we wish to communicate with the docker swarm on port <code>3376</code> and not the local docker engine on port <code>2376</code>.</em></p>
<p>Docker swarm has the same API as the normal docker engine, so we can run containers on the swarm in the same way that we run containers on the local docker engine:</p>
<pre><code>$ docker run hello-world
Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

 To try something more ambitious, you can run an Ubuntu container with:
  $ docker run -it ubuntu bash

 Share images, automate workflows, and more with a free Docker Hub account:
  https://hub.docker.com

 For more examples and ideas, visit:
  https://docs.docker.com/engine/userguide/</code></pre>
<p>Now we will examine where this ran. run <code>docker ps -a</code> and you will see an output similar to this, which shows that hello-world was run on a specific node:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">docker</span> ps -a
<span class="kw">CONTAINER</span> ID        IMAGE                COMMAND                CREATED             STATUS                     PORTS                                     NAMES
<span class="kw">54a8690043dd</span>        hello-world:latest   <span class="st">&quot;/hello&quot;</span>               22 seconds ago      Exited (0) <span class="kw">3</span> seconds ago                                             worker-1/modest_goodall     
<span class="kw">78be991b58d1</span>        swarm:latest         <span class="st">&quot;/swarm join --addr    5 minutes ago       Up 4 minutes               2375/tcp                                  worker-2/swarm-agent        </span>
<span class="st">da5127e4f0f9        swarm:latest         &quot;</span>/swarm join --addr    8 minutes ago       Up 8 minutes               2375/tcp                                  worker-1/swarm-agent                
<span class="kw">45821ca5208e</span>        swarm:latest         <span class="st">&quot;/swarm manage --tls   18 minutes ago      Up 18 minutes              2375/tcp, 192.168.99.104:3376-&gt;3376/tcp   swarm-master/swarm-agent-master   </span></code></pre></div>
<p>Notice the output, which is different from that generated when we run the command against the local docker engine. Here the listing includes information about execution over docker hosts in the pool. Try running a few more docker containers and observe the evolution of the container deployment.Notice also the swarm containers running on our nodes. We ran these up earlier in the process.</p>
</body>
</html>
